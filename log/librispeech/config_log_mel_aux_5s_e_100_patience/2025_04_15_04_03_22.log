2025-04-15 04:03:22,540,root,INFO,logger initialized
2025-04-15 04:03:22,556,root,INFO,logger initialized
2025-04-15 04:03:22,566,root,INFO,logger initialized
2025-04-15 04:03:22,598,root,INFO,logger initialized
2025-04-15 04:03:22,598,root,INFO,logging initialized succesully
2025-04-15 04:03:22,598,root,INFO,AttrDict(log='log/librispeech/config_log_mel_aux_5s_e_100_patience', config='exp/librispeech/config_log_mel_aux_5s_e_100_patience.yaml', ckpt_path='ckpt/librispeech/config_log_mel_aux_5s_e_100_patience', resume='', fine_tune=None, world_size=4, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1, init_param=['/DKUdata/tangbl/FunCodec/egs/LibriTTS/codec/exp/audio_codec-encodec-zh_en-general-16k-nq32ds640-pytorch/model.pth:quantizer.rq.model:quantizer_codebook'], codec_model_file='/DKUdata/tangbl/FunCodec/egs/LibriTTS/codec/exp/audio_codec-encodec-zh_en-general-16k-nq32ds640-pytorch/model.pth', codec_config_file='/DKUdata/tangbl/FunCodec/egs/LibriTTS/codec/exp/audio_codec-encodec-zh_en-general-16k-nq32ds640-pytorch/config.yaml', train_shape_file=['/DKUdata/tangbl/data/librispeech/funcodec/data/train/all_shape.scp'], valid_shape_file=['/DKUdata/tangbl/laura_gpt_se/libri2mix_tse_data_funcodec/s1/dev/all_shape.scp'], codec_hop_size=640, audio_fs=16000, spk_dict_path='/DKUdata/tangbl/data/librispeech/train_clean_100_360.pkl', max_mix_ds=15, max_aux_ds=5, train_data_path_and_name_and_type=[['/DKUdata/tangbl/data/librispeech/clean.scp', 'text', 'dm_libri_mix'], ['/DKUdata/tangbl/data/librispeech/clean.scp', 'aux', 'dm_libri_ref'], ['/DKUdata/tangbl/data/librispeech/funcodec/data/train/all.scp', 'codec', 'npy']], valid_data_path_and_name_and_type=[['/Netdata/2021/zb/data/LibriMix/Libri2Mix/wav16k/min/lists/dev/mix.scp', 'raw', 'sound'], ['/Netdata/2021/zb/data/LibriMix/Libri2Mix/wav16k/min/lists/dev/aux_s1.scp', 'raw_aux', 'sound'], ['/DKUdata/tangbl/laura_gpt_se/libri2mix_tse_data_funcodec/s1/dev/all.scp', 'codec', 'npy']], grad_clip=5, seed=1234, init=None, input_size=128, use_preprocessor=False, audio_max_duration=60, codec_token_rate=25, text_encoder='conformer', text_encoder_conf={'output_size': 512, 'attention_heads': 8, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'linear', 'normalize_before': True, 'rel_pos_type': 'latest', 'pos_enc_layer_type': 'rel_pos', 'selfattention_layer_type': 'rel_selfattn', 'use_cnn_module': False}, codec_encoder='conformer', codec_encoder_conf={'output_size': 512, 'attention_heads': 8, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'linear', 'normalize_before': True, 'rel_pos_type': 'latest', 'pos_enc_layer_type': 'rel_pos', 'selfattention_layer_type': 'rel_selfattn', 'use_cnn_module': False}, model='laura_gen_model', model_conf={'codec_sampling_ratio': 0.5, 'lsm_weight': 0.0, 'length_normalized_loss': True, 'predict_nq': 2, 'codec_conf': {'num_quantizers': 32, 'codebook_size': 1024, 'codebook_dim': 128}, 'codec_lm_conf': {'name': 'transformer', 'pos_enc': 'rel_pos', 'selfattention_layer_type': 'rel_selfattn', 'embed_unit': 128, 'att_unit': 512, 'head': 8, 'unit': 2048, 'layer': 10, 'dropout_rate': 0.1, 'pe_type': 'uni', 'bidirectional_inputs': True, 'codec_groups': 1}}, batch_type='length', batch_bins=7680, batch_size=40, sort_in_batch='descending', sort_batch='descending', num_workers=8, max_cache_size=0.0, max_cache_fd=32, train_dtype='float32', allow_variable_data_keys=True, drop_last=False, fold_length=[], mel_config={'n_fft': 512, 'hop_size': 256, 'log_mel': True}, optim={'type': 'Adam', 'args': {'lr': 0.001}}, scheduler='warmuplr', scheduler_conf={'warmup_steps': 10000}, best_field='loss', best_save_type='descend', max_ckpt=1, log_interval=10, epoch=50, ngpu=4)
2025-04-15 04:03:22,600,root,INFO,rank 0 of world_size 4 started...
2025-04-15 04:03:22,600,root,INFO,setup model
2025-04-15 04:03:24,534,root,INFO,encoder self-attention layer type = relative self-attention
2025-04-15 04:03:24,596,root,INFO,encoder self-attention layer type = relative self-attention
2025-04-15 04:03:24,611,root,INFO,encoder self-attention layer type = relative self-attention
2025-04-15 04:03:24,619,root,INFO,encoder self-attention layer type = relative self-attention
2025-04-15 04:03:24,847,root,INFO,Using distributed residual vector quantization.
2025-04-15 04:03:24,879,root,INFO,Using distributed residual vector quantization.
2025-04-15 04:03:24,895,root,INFO,Using distributed residual vector quantization.
2025-04-15 04:03:24,906,root,INFO,Using distributed residual vector quantization.
2025-04-15 04:03:25,573,root,INFO,model LauraTSE(
  (pos_emb_func): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (text_encoder): ConformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=128, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): EncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): EncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
  )
  (text_enc_out_layer): Linear(in_features=512, out_features=128, bias=True)
  (lm_embedding): Embedding(3, 128)
  (codec_lm): TransformerEmbedLM(
    (encoder): TransformerEncoder_s0(
      (embed): Sequential(
        (0): Linear(in_features=128, out_features=512, bias=True)
        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (2): Dropout(p=0.1, inplace=False)
        (3): ReLU()
        (4): RelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (encoders): MultiSequential(
        (0): EncoderLayer(
          (self_attn): RelPositionMultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear_pos): Linear(in_features=512, out_features=512, bias=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): EncoderLayer(
          (self_attn): RelPositionMultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear_pos): Linear(in_features=512, out_features=512, bias=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): EncoderLayer(
          (self_attn): RelPositionMultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear_pos): Linear(in_features=512, out_features=512, bias=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): EncoderLayer(
          (self_attn): RelPositionMultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear_pos): Linear(in_features=512, out_features=512, bias=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): EncoderLayer(
          (self_attn): RelPositionMultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear_pos): Linear(in_features=512, out_features=512, bias=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): EncoderLayer(
          (self_attn): RelPositionMultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear_pos): Linear(in_features=512, out_features=512, bias=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (6): EncoderLayer(
          (self_attn): RelPositionMultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear_pos): Linear(in_features=512, out_features=512, bias=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (7): EncoderLayer(
          (self_attn): RelPositionMultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear_pos): Linear(in_features=512, out_features=512, bias=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (8): EncoderLayer(
          (self_attn): RelPositionMultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear_pos): Linear(in_features=512, out_features=512, bias=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (9): EncoderLayer(
          (self_attn): RelPositionMultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (linear_pos): Linear(in_features=512, out_features=512, bias=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
    )
    (decoder): Linear(in_features=512, out_features=2050, bias=True)
  )
  (codec_encoder): ConformerEncoder(
    (embed): Sequential(
      (0): Linear(in_features=128, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.1, inplace=False)
      (3): RelPositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): EncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): EncoderLayer(
        (self_attn): RelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=512, out_features=512, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
  )
  (codec_encoder_out_layer): Linear(in_features=512, out_features=128, bias=True)
  (quantizer_codebook): QuantizerCodebook()
  (criterion_ce): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (quantizer): CostumeQuantizer(
    (rq): ResidualVectorQuantizer(
      (model): DistributedResidualVectorQuantization(
        (layers): ModuleList(
          (0-31): 32 x VectorQuantization(
            (project_in): Identity()
            (project_out): Identity()
            (_codebook): EuclideanCodebook()
          )
        )
      )
    )
  )
) is intialized
2025-04-15 04:03:25,579,root,INFO,model parameters: 76529794
2025-04-15 04:03:25,580,root,INFO,auto-regressive decoder-only LM parameters: 35275266
2025-04-15 04:03:25,580,root,INFO,Loading pretrained params from /DKUdata/tangbl/FunCodec/egs/LibriTTS/codec/exp/audio_codec-encodec-zh_en-general-16k-nq32ds640-pytorch/model.pth:quantizer.rq.model:quantizer_codebook
2025-04-15 04:03:29,858,root,WARNING,Filter out inited from pretrained dict because of name not found in target dict
2025-04-15 04:03:29,858,root,WARNING,Filter out cluster_size from pretrained dict because of name not found in target dict
2025-04-15 04:03:29,858,root,WARNING,Filter out embed_avg from pretrained dict because of name not found in target dict
2025-04-15 04:03:29,858,root,INFO,Loaded src_state keys: dict_keys(['embed'])
2025-04-15 04:03:29,859,root,WARNING,Filter out inited from pretrained dict because of name not found in target dict
2025-04-15 04:03:29,860,root,WARNING,Filter out cluster_size from pretrained dict because of name not found in target dict
2025-04-15 04:03:29,860,root,WARNING,Filter out embed_avg from pretrained dict because of name not found in target dict
2025-04-15 04:03:29,861,root,INFO,Loaded src_state keys: dict_keys(['embed'])
2025-04-15 04:03:29,871,root,WARNING,Filter out inited from pretrained dict because of name not found in target dict
2025-04-15 04:03:29,871,root,WARNING,Filter out cluster_size from pretrained dict because of name not found in target dict
2025-04-15 04:03:29,871,root,WARNING,Filter out embed_avg from pretrained dict because of name not found in target dict
2025-04-15 04:03:29,871,root,INFO,Loaded src_state keys: dict_keys(['embed'])
2025-04-15 04:03:29,872,root,INFO,# FINE TUNING # from ckpt None
2025-04-15 04:03:29,884,root,WARNING,Filter out inited from pretrained dict because of name not found in target dict
2025-04-15 04:03:29,885,root,WARNING,Filter out cluster_size from pretrained dict because of name not found in target dict
2025-04-15 04:03:29,885,root,WARNING,Filter out embed_avg from pretrained dict because of name not found in target dict
2025-04-15 04:03:29,885,root,INFO,Loaded src_state keys: dict_keys(['embed'])
