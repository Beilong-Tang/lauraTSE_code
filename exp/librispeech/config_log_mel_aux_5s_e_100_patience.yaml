############
# FunCodec #
############


# For inference need
codec_model_file: /DKUdata/tangbl/FunCodec/egs/LibriTTS/codec/exp/audio_codec-encodec-zh_en-general-16k-nq32ds640-pytorch/model.pth
codec_config_file: /DKUdata/tangbl/FunCodec/egs/LibriTTS/codec/exp/audio_codec-encodec-zh_en-general-16k-nq32ds640-pytorch/config.yaml

########
# Data #
########

train_shape_file: ["/DKUdata/tangbl/data/librispeech/funcodec/data/train/all_shape.scp"]
valid_shape_file: ["/DKUdata/tangbl/laura_gpt_se/libri2mix_tse_data_funcodec/s1/dev/all_shape.scp"]
spk_dict_path: /DKUdata/tangbl/data/librispeech/train_clean_100_360.pkl
train_data_path_and_name_and_type: [
    [
        "/DKUdata/tangbl/data/librispeech/clean.scp",
        "text",
        "dm_mix"
    ],
    [
        "/DKUdata/tangbl/data/librispeech/clean.scp",
        "aux",
        "dm_ref"
    ],
    [
        "/DKUdata/tangbl/data/librispeech/funcodec/data/train/all.scp",
        "codec",
        "npy"
    ]
]

valid_data_path_and_name_and_type: [
     [
        "/Netdata/2021/zb/data/LibriMix/Libri2Mix/wav16k/min/lists/dev/mix.scp",
        "text",
        "mix_mel"
    ],
    [
        "/Netdata/2021/zb/data/LibriMix/Libri2Mix/wav16k/min/lists/dev/aux_s1.scp",
        "aux",
        "ref_mel"
    ],
    [
        "/DKUdata/tangbl/laura_gpt_se/libri2mix_tse_data_funcodec/s1/dev/all.scp",
        "codec",
        "npy"
    ]
]
codec_hop_size: 640
audio_fs: 16000
max_mix_ds: 15
max_aux_ds: 5

############
# Training #
############

best_field: loss
best_save_type: descend
max_ckpt: 1
log_interval: 10
epoch: 100

batch_bins: 7680 # for 4 gpu cards with 46Gb each. You can modify this based on the gpu bandwidth.
batch_size: 40 # This does not matter here
num_workers: 8

# Optimizer
optim:
    type: Adam
    args:
        lr: 1.0e-3

# Warmup Scheduler
scheduler: warmuplr
scheduler_conf:
    warmup_steps: 10000

## patience scheduler
patience:
    epoch: 50 # the epoch to start patience scheduler

grad_clip: 5
seed: 1234
init: null

input_size: 128 
use_preprocessor: False
audio_max_duration: 60
codec_token_rate: 25

# Don't change #
batch_type: length
sort_in_batch: descending
sort_batch: descending
max_cache_size: 0.0
max_cache_fd: 32
train_dtype: float32
## Add for argument type checking
allow_variable_data_keys: true
drop_last: false
fold_length: []



#########
# Model #
#########

### Mel config ###
mel_config:
  n_fft: 512
  hop_size: 256
  log_mel: True


### Conformer Encoder
text_encoder: conformer
text_encoder_conf:
    output_size: 512    # dimension of attention
    attention_heads: 8
    linear_units: 2048  # the number of units of position-wise feed forward
    num_blocks: 6      # the number of encoder blocks
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    attention_dropout_rate: 0.0
    input_layer: linear # encoder architecture type
    normalize_before: true
    rel_pos_type: latest
    pos_enc_layer_type: rel_pos
    selfattention_layer_type: rel_selfattn
    use_cnn_module: false

### Encoder-only LM
codec_encoder: conformer
codec_encoder_conf:
    output_size: 512    # dimension of attention
    attention_heads: 8
    linear_units: 2048  # the number of units of position-wise feed forward
    num_blocks: 6      # the number of encoder blocks
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    attention_dropout_rate: 0.0
    input_layer: linear # encoder architecture type
    normalize_before: true
    rel_pos_type: latest
    pos_enc_layer_type: rel_pos
    selfattention_layer_type: rel_selfattn
    use_cnn_module: false

# Auto-regressive Decoder-only LM
model: laura_gen_model
model_conf:
    codec_sampling_ratio: 0.5
    lsm_weight: 0.0
    length_normalized_loss: true
    predict_nq: 2
    codec_conf:
        num_quantizers: 32
        codebook_size: 1024
        codebook_dim: 128
    codec_lm_conf:
        name: transformer
        pos_enc: rel_pos
        selfattention_layer_type: rel_selfattn
        embed_unit: 128
        att_unit: 512
        head: 8
        unit: 2048
        # layer: 12
        layer: 10
        dropout_rate: 0.1
        pe_type: uni
        bidirectional_inputs: true
        codec_groups: 1